{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from keras.layers import Embedding, LSTM, GRU, Dropout, Dense, Input\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.layers import Input, Flatten, Dropout, Dense, Embedding, Conv1D, MaxPooling1D, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') #下載stopwords辭典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text): #將完整語句切割成個別單字，剃除停止詞\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') \n",
    "basepath = r'D:\\jupyter\\training_dataset_v1\\training_dataset_v1\\training_dataset'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-89b4cce1692f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbasepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\codecs.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[0mbyte\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;31m# undecoded input that is kept between calls to decode()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "#每個檔案彙整，切割成個別單字後。全部儲存成csv檔案，方便之後資料IO\n",
    "\n",
    "for l in ('pos', 'neg'):\n",
    "    path = os.path.join(basepath, l)\n",
    "    for file in os.listdir(path):\n",
    "        with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "            txt = infile.read()\n",
    "            token = tokenizer(text=txt)\n",
    "        df = df.append([[token, labels[l]]], ignore_index=True)\n",
    "        pbar.update()\n",
    "df.columns = ['review', 'sentiment']\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import gensim.models\n",
    "import re\n",
    "\n",
    "inpath = 'movie_data.csv'\n",
    "outpath = 'wordVectTrainResult'\n",
    "pbar = pyprind.ProgBar(100000)\n",
    "class csvStream(object):\n",
    "    def __init__(self,path):\n",
    "        self.path=path\n",
    "    def __iter__(self):\n",
    "        with open(self.path, 'r',encoding=\"utf-8\") as csv:\n",
    "            next(csv)  # skip header\n",
    "            for line in csv:\n",
    "                text = line[4:-3]\n",
    "                text = re.sub('[\\'\\\"\\[\\]\\d\\b]','',text)   \n",
    "                while (text[0] == ',') or (text[0] == ' '):\n",
    "                    text = text[1:]\n",
    "                pbar.update()\n",
    "                yield text.split(', ')\n",
    "\n",
    "#製作字典，資料以評論所出現過的單字進行整理，並依照出現頻率由大到小排序\n",
    "lineIterator = csvStream(inpath)\n",
    "model = gensim.models.Word2Vec()\n",
    "model.build_vocab(lineIterator)\n",
    "print('vocabulary building finished, start training...')\n",
    "model.train(lineIterator,total_examples=model.corpus_count,epochs=20)\n",
    "model.save(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取字典\n",
    "inpath = 'wordVectTrainResult'\n",
    "model = gensim.models.Word2Vec.load(inpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立轉換資料矩陣，以及餵入模型中的鑲嵌層\n",
    "def build_word2idx_embedMatrix(w2vModel):\n",
    "    word2idx = {\"_stopWord\": 0}  # 这里加了一行是用来过滤停用词的。\n",
    "    vocab_list = [(w, w2vModel.wv[w]) for w, v in w2vModel.wv.vocab.items()]\n",
    "    embedMatrix = np.zeros((len(w2vModel.wv.vocab.items()) + 1, w2vModel.vector_size))#+1是因为停用词\n",
    "    for i in range(0, len(vocab_list)):\n",
    "        word = vocab_list[i][0]#vocab_list[i][0]是词\n",
    "        word2idx[word] = i + 1#因为加了停用词，所以其他索引都加1\n",
    "        embedMatrix[i + 1] = vocab_list[i][1]#vocab_list[i][1]这个词对应embedding_matrix的那一行\n",
    "    return word2idx, embedMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx, embedMatrix = build_word2idx_embedMatrix(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 500, 100)          2861300   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 500, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 500, 64)           32064     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               640100    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "sigmoe (Activation)          (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,533,565\n",
      "Trainable params: 672,265\n",
      "Non-trainable params: 2,861,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#神經網路模型\n",
    "sequence_input = Input(shape=(500,), dtype=np.int32)\n",
    "embedding_layer = Embedding(input_dim=len(embedMatrix), output_dim=len(embedMatrix[0]),\n",
    "                                weights=[embedMatrix],  # 表示直接使用预训练的词向量\n",
    "                                trainable=False)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Dropout(.25)(embedded_sequences)\n",
    "x = Conv1D(64, 5, padding='same', activation='relu')(x)\n",
    "x = Dropout(.25)(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dropout(.7)(x)\n",
    "y = Dense(1, activation='sigmoid')(x)\n",
    "y = keras.layers.Activation(activation=keras.activations.sigmoid,name=\"sigmoe\")(y)\n",
    "model = Model(sequence_input, y)\n",
    "model.summary()\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將文字矩陣，轉換成數字矩陣，取每個資料中，在字典出現頻率最高前500字\n",
    "def make_deepLearn_data(sentenList, word2idx):\n",
    "    X_train_idx = [[word2idx.get(w, 0) for w in sen] for sen in sentenList]#之前都是通过word处理的，这里word2idx讲word转为id\n",
    "    X_train_idx = np.array(sequence.pad_sequences(X_train_idx, maxlen=500))  # padding成相同长度\n",
    "    return X_train_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#整理資料\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r',encoding=\"utf-8\") as csv:\n",
    "        next(csv)  # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[4:-3], int(line[-2])\n",
    "            text = re.sub('[\\'\\\"\\[\\]\\d\\b]','',text)\n",
    "            while text[0] == ',':\n",
    "                    text = text[1:]\n",
    "            yield text.split(', '), label\n",
    "\n",
    "#取出特定筆數資料\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#訓練23000筆 驗證2000筆\n",
    "BatchSize = 1000\n",
    "BatchNum = 23\n",
    "doc_stream = stream_docs(path='movie_data.csv') \n",
    "X_raw, y_train = get_minibatch(doc_stream, size=BatchSize*BatchNum)\n",
    "X_raw_test, y_test = get_minibatch(doc_stream, size=(25000-BatchNum*BatchSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文字矩陣轉數字矩陣，才可讓模型訓練\n",
    "x_data = make_deepLearn_data(X_raw,word2idx)\n",
    "x_test = make_deepLearn_data(X_raw_test,word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "23000/23000 [==============================] - 26s 1ms/step - loss: 0.6934 - acc: 0.5082 - val_loss: 0.6931 - val_acc: 0.5125\n",
      "Epoch 2/20\n",
      "23000/23000 [==============================] - 26s 1ms/step - loss: 0.6934 - acc: 0.5066 - val_loss: 0.6931 - val_acc: 0.5090\n",
      "Epoch 3/20\n",
      "23000/23000 [==============================] - 24s 1ms/step - loss: 0.6932 - acc: 0.5036 - val_loss: 0.6931 - val_acc: 0.5095\n",
      "Epoch 4/20\n",
      "23000/23000 [==============================] - 25s 1ms/step - loss: 0.6912 - acc: 0.5163 - val_loss: 0.6742 - val_acc: 0.6205\n",
      "Epoch 5/20\n",
      "23000/23000 [==============================] - 24s 1ms/step - loss: 0.6116 - acc: 0.7589 - val_loss: 0.5715 - val_acc: 0.8320\n",
      "Epoch 6/20\n",
      "23000/23000 [==============================] - 25s 1ms/step - loss: 0.5861 - acc: 0.8239 - val_loss: 0.5702 - val_acc: 0.8645\n",
      "Epoch 7/20\n",
      "23000/23000 [==============================] - 25s 1ms/step - loss: 0.5769 - acc: 0.8401 - val_loss: 0.5714 - val_acc: 0.8455\n",
      "Epoch 8/20\n",
      "23000/23000 [==============================] - 24s 1ms/step - loss: 0.5737 - acc: 0.8491 - val_loss: 0.5624 - val_acc: 0.8720\n",
      "Epoch 9/20\n",
      "23000/23000 [==============================] - 24s 1ms/step - loss: 0.5730 - acc: 0.8474 - val_loss: 0.5708 - val_acc: 0.8420\n",
      "Epoch 10/20\n",
      "23000/23000 [==============================] - 25s 1ms/step - loss: 0.5711 - acc: 0.8530 - val_loss: 0.5650 - val_acc: 0.8690\n",
      "Epoch 11/20\n",
      "23000/23000 [==============================] - 25s 1ms/step - loss: 0.5718 - acc: 0.8533 - val_loss: 0.5617 - val_acc: 0.8800\n",
      "Epoch 12/20\n",
      "23000/23000 [==============================] - 25s 1ms/step - loss: 0.5686 - acc: 0.8607 - val_loss: 0.5643 - val_acc: 0.8725\n",
      "Epoch 13/20\n",
      "23000/23000 [==============================] - 25s 1ms/step - loss: 0.5693 - acc: 0.8577 - val_loss: 0.5655 - val_acc: 0.8700\n",
      "Epoch 14/20\n",
      "23000/23000 [==============================] - 25s 1ms/step - loss: 0.5680 - acc: 0.8597 - val_loss: 0.5643 - val_acc: 0.8700\n",
      "Epoch 15/20\n",
      "23000/23000 [==============================] - 25s 1ms/step - loss: 0.5678 - acc: 0.8602 - val_loss: 0.5656 - val_acc: 0.8715\n",
      "Epoch 16/20\n",
      "23000/23000 [==============================] - 24s 1ms/step - loss: 0.5664 - acc: 0.8664 - val_loss: 0.5646 - val_acc: 0.8670\n",
      "Epoch 17/20\n",
      "23000/23000 [==============================] - 24s 1ms/step - loss: 0.5660 - acc: 0.8637 - val_loss: 0.5637 - val_acc: 0.8810\n",
      "Epoch 18/20\n",
      "23000/23000 [==============================] - 24s 1ms/step - loss: 0.5676 - acc: 0.8615 - val_loss: 0.5625 - val_acc: 0.8815\n",
      "Epoch 19/20\n",
      "23000/23000 [==============================] - 24s 1ms/step - loss: 0.5672 - acc: 0.8627 - val_loss: 0.5643 - val_acc: 0.8755\n",
      "Epoch 20/20\n",
      "23000/23000 [==============================] - 24s 1ms/step - loss: 0.5680 - acc: 0.8607 - val_loss: 0.5644 - val_acc: 0.8785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14aef181608>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_data,y=y_train,epochs=20, batch_size=100, verbose=1,validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將模型存檔\n",
    "model.save('imdb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('imdb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_predict(filepath,word2idx,model):\n",
    "    with open(filepath, 'r', encoding='utf-8') as infile:\n",
    "        txt = infile.read()\n",
    "        token = [tokenizer(text=txt)]\n",
    "        data=make_deepLearn_data(token,word2idx)\n",
    "        return(model.predict(data))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取字典\n",
    "inpath = 'wordVectTrainResult'\n",
    "dic = gensim.models.Word2Vec.load(inpath)\n",
    "word2idx, embedMatrix = build_word2idx_embedMatrix(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#大於0.5判為 pos 小於0.5 判為neg\n",
    "text_predict(r'D:\\jupyter\\training_dataset_v1\\training_dataset_v1\\training_dataset\\pos\\0_9.txt',word2idx,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#建立轉換資料矩陣，以及餵入模型中的鑲嵌層\n",
    "def build_word2idx_embedMatrix(w2vModel):\n",
    "    word2idx = {\"_stopWord\": 0}  # 这里加了一行是用来过滤停用词的。\n",
    "    vocab_list = [(w, w2vModel.wv[w]) for w, v in w2vModel.wv.vocab.items()]\n",
    "    embedMatrix = np.zeros((len(w2vModel.wv.vocab.items()) + 1, w2vModel.vector_size))#+1是因为停用词\n",
    "    for i in range(0, len(vocab_list)):\n",
    "        word = vocab_list[i][0]#vocab_list[i][0]是词\n",
    "        word2idx[word] = i + 1#因为加了停用词，所以其他索引都加1\n",
    "        embedMatrix[i + 1] = vocab_list[i][1]#vocab_list[i][1]这个词对应embedding_matrix的那一行\n",
    "    return word2idx, embedMatrix\n",
    "def tokenizer(text): #將完整語句切割成個別單字，剃除停止詞\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "#將文字矩陣，轉換成數字矩陣，取每個資料中，在字典出現頻率最高前500字\n",
    "def make_deepLearn_data(sentenList, word2idx):\n",
    "    X_train_idx = [[word2idx.get(w, 0) for w in sen] for sen in sentenList]#之前都是通过word处理的，这里word2idx讲word转为id\n",
    "    X_train_idx = np.array(sequence.pad_sequences(X_train_idx, maxlen=500))  # padding成相同长度\n",
    "    return X_train_idx\n",
    "stop = stopwords.words('english') \n",
    "model = load_model('imdb.h5')\n",
    "#讀取字典\n",
    "inpath = 'wordVectTrainResult'\n",
    "dic = gensim.models.Word2Vec.load(inpath)\n",
    "word2idx, embedMatrix = build_word2idx_embedMatrix(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv= pd.read_csv('test_dataset.csv',header=None,encoding=\"utf-8\")\n",
    "csv = csv[1].tolist()\n",
    "test_data=[]\n",
    "count=1\n",
    "for line in csv:\n",
    "    if str(line)==\"nan\":\n",
    "        test_data.append(\"\")\n",
    "    else:\n",
    "        test_data.append(tokenizer(line))\n",
    "    count=count+1\n",
    "x_test = make_deepLearn_data(test_data,word2idx)\n",
    "answer = model.predict(x_test)\n",
    "answer = answer.tolist()\n",
    "status = [1 if value[0] >0.5 else 0 for value in answer]\n",
    "df=pd.DataFrame(range(1,len(status)+1))\n",
    "df.columns = ['Id']\n",
    "df['Label'] = status\n",
    "df.to_csv('submission.csv',index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
